{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import misc\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    " \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.00001\n",
    "batch_size = 32\n",
    "display_step = batch_size*10\n",
    "hm_epochs=30\n",
    "BS=batch_size\n",
    "picsize=256\n",
    "inputdepth=7\n",
    "stvs=0.01\n",
    "trainmode=True\n",
    "localpath='/path/to/dir/Knee_MRI/Femur/Train/'\n",
    "\n",
    "num_input = picsize*picsize \n",
    "num_classes = 2 \n",
    "\n",
    "# tf Graph input\n",
    "with tf.name_scope(\"Input\"):\n",
    "    X = tf.placeholder(tf.float32, [None, picsize,picsize,inputdepth])\n",
    "with tf.name_scope(\"True_Class\"):\n",
    "    Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "with tf.name_scope(\"Blanks\"):\n",
    "    num_classes_tf=tf.constant(num_classes,tf.int32)\n",
    "with tf.name_scope(\"true_labels\"):\n",
    "    true_label=tf.placeholder(tf.float32, [1])\n",
    "\n",
    "    \n",
    "# Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. \n",
    "# arXiv preprint arXiv:1409.1556. 2014 Sep 4.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image store\n",
    "runmode='jupyter'\n",
    "\n",
    "if runmode=='jupyter':\n",
    "    imdpath=localpath + \"ImdTrain/\"\n",
    "    imdfiles=glob.glob(localpath + \"ImdTrain/*.png\")\n",
    "    numfiles=int(len(glob.glob(localpath \"ImdTrain/*.png\"))/inputdepth)\n",
    "    root=localpath\n",
    "    \n",
    "    imdpath=localpath + \"ImdVal/\"\n",
    "    imdfiles=glob.glob(localpath + \"ImdVal/*.png\")\n",
    "    numfiles=int(len(glob.glob(localpath \"ImdVal/*.png\"))/inputdepth)\n",
    "    root=localpath\n",
    "    \n",
    "else: \n",
    "    imdpath=\"/mydata/Femur/Train/ImdTrain/\"\n",
    "    imdfiles=glob.glob(\"/mydata/Femur/Train/ImdTrain/*.png\")\n",
    "    numfiles=int(len(glob.glob(\"/mydata/Femur/Train/ImdTrain/*.png\"))/inputdepth)\n",
    "    root='/mydata/'\n",
    "    \n",
    "    imdpath_val=\"/mydata/Femur/Val/ImdVal/\"\n",
    "    imdfiles_val=glob.glob(\"/mydata/Femur/Val/ImdVal/*.png\")\n",
    "    numfiles_val=int(len(glob.glob(\"/mydata/Femur/Val/ImdVal/*.png\"))/inputdepth)\n",
    "    root_val='/mydata/'\n",
    "    \n",
    "\n",
    "print(numfiles)\n",
    "\n",
    "x = ([[i] for i in range(numfiles)])\n",
    "shuflist=random.sample(x,len(x))\n",
    "\n",
    "#print(imdfiles[0])\n",
    "#print(pxdfiles[0])\n",
    "tempfiles_new=[]\n",
    "\n",
    "s=''\n",
    "for i in range(numfiles):\n",
    "    if len(str(shuflist[i][0]+1))==1:\n",
    "        templist=('0000', str(shuflist[i][0]+1)) \n",
    "        tempfiles_new.append(s.join(templist))\n",
    "    elif len(str(shuflist[i][0]+1))==2:\n",
    "        templist=('000', str(shuflist[i][0]+1)) \n",
    "        tempfiles_new.append(s.join(templist)) \n",
    "    elif len(str(shuflist[i][0]+1))==3:\n",
    "        templist=('00', str(shuflist[i][0]+1)) \n",
    "        tempfiles_new.append(s.join(templist))  \n",
    "    elif len(str(shuflist[i][0]+1))==4:\n",
    "        templist=('0', str(shuflist[i][0]+1)) \n",
    "        tempfiles_new.append(s.join(templist)) \n",
    "    else:\n",
    "        templist=(str(shuflist[i][0]+1)) \n",
    "        tempfiles_new.append(s.join(templist))\n",
    "        \n",
    "print(tempfiles_new[:10])\n",
    "print((imdpath + str(tempfiles_new[0]) + 'A.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class data for classification of slices is stored as either 0 or 1 (not a femur, or femur) in a .csv \n",
    "#where each row corresponds to slice's file identifier in the data store\n",
    "\n",
    "classtrue=[]\n",
    "path=(root + \"GroundList.csv\")\n",
    "print(path)\n",
    "f = open(path, 'r')\n",
    "reader = csv.reader(f)\n",
    "for row in reader:\n",
    "    classtrue.append(row)\n",
    "f.close()\n",
    "\n",
    "print(len(classtrue))\n",
    "print(classtrue[:10])\n",
    "print(int(classtrue[3][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classtrue_val=[]\n",
    "path=(root + \"Femur/Val/fem_mri_classlist_Val.csv\")\n",
    "print(path)\n",
    "f = open(path, 'r')\n",
    "reader = csv.reader(f)\n",
    "for row in reader:\n",
    "    classtrue_val.append(row)\n",
    "f.close()\n",
    "\n",
    "print(len(classtrue_val))\n",
    "print(classtrue_val[:10])\n",
    "print(int(classtrue_val[3][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(inputs, channels_in, channels_out,is_training,stvs,strides=1,scopename=\"conv\"):\n",
    "    with tf.name_scope(scopename):\n",
    "        w=tf.Variable(tf.random_normal([3, 3, channels_in, channels_out],stddev=stvs))\n",
    "        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, w)\n",
    "            \n",
    "        b=tf.Variable(tf.random_normal([channels_out],stddev=stvs))\n",
    "        tf.summary.histogram((\"Weight\" + scopename),w)\n",
    "        tf.summary.histogram((\"Bias\" + scopename),b)\n",
    "        x = tf.nn.conv2d(inputs, w, strides=[1, strides, strides, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        x=batch_norm_wrapper(x, is_training, decay = 0.99)\n",
    "        return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2,scopename=\"pool\"):\n",
    "    with tf.name_scope(scopename):\n",
    "        # MaxPool2D wrapper\n",
    "        return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='VALID')\n",
    "    \n",
    "# Batch normalization wrapper to distinguish between training and testing\n",
    "# SOURCE: https://r2rt.com/implementing-batch-normalization-in-tensorflow.html\n",
    "def batch_norm_wrapper(inputs, is_training, decay = 0.99):\n",
    "    \n",
    "    epsilon = 1e-3\n",
    "    scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "    beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    if is_training:\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs,[0,1,2])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs,\n",
    "                batch_mean, batch_var, beta, scale, epsilon)\n",
    "    else:\n",
    "        return tf.nn.batch_normalization(inputs,\n",
    "            pop_mean, pop_var, beta, scale, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Architecture\n",
    "def conv_net(x, inputdepth, is_training):\n",
    "    stvs=0.01\n",
    "    depthstart=64\n",
    "    num_classes=2\n",
    "    if is_training==True:\n",
    "        keepprob=0.5\n",
    "    else:\n",
    "        keepprob=1\n",
    "    \n",
    "    #Fit batches into correct shape\n",
    "    x = tf.reshape(x, shape=[-1, picsize, picsize, inputdepth]) \n",
    "    \n",
    "    conv1a = conv_layer(x, inputdepth, depthstart, is_training, stvs,scopename=\"conv1a\")\n",
    "    conv1b = conv_layer(conv1a, depthstart, depthstart, is_training, stvs, scopename=\"conv1b\")\n",
    "    pooled1 = maxpool2d(conv1b,scopename=\"pooled1\")\n",
    "        \n",
    "    conv2a = conv_layer(pooled1, depthstart, depthstart*2, is_training, stvs,scopename=\"conv2a\")\n",
    "    conv2b = conv_layer(conv2a, depthstart*2, depthstart*2, is_training, stvs, scopename=\"conv2b\")\n",
    "    pooled2 = maxpool2d(conv2b,scopename=\"pooled2\")\n",
    "    \n",
    "    conv3a = conv_layer(pooled2, depthstart*2, depthstart*4, is_training, stvs,scopename=\"conv3a\")\n",
    "    conv3b = conv_layer(conv3a, depthstart*4, depthstart*4, is_training, stvs, scopename=\"conv3b\")\n",
    "    conv3c = conv_layer(conv3b, depthstart*4, depthstart*4, is_training, stvs, scopename=\"conv3c\")\n",
    "    pooled3 = maxpool2d(conv3c,scopename=\"pooled3\")\n",
    "    \n",
    "    conv4a = conv_layer(pooled3, depthstart*4, depthstart*8, is_training, stvs,scopename=\"conv4a\")\n",
    "    conv4b = conv_layer(conv4a, depthstart*8, depthstart*8, is_training, stvs, scopename=\"conv4b\")\n",
    "    conv4c = conv_layer(conv4b, depthstart*8, depthstart*8, is_training, stvs, scopename=\"conv4c\")\n",
    "    pooled4 = maxpool2d(conv4c,scopename=\"pooled4\")\n",
    "    \n",
    "    #Dense layers\n",
    "    with tf.name_scope(\"Dense1\"):\n",
    "        dense1nodes=1000\n",
    "        flatten=tf.reshape(pooled4,[-1,(depthstart*8)*16*16])\n",
    "        dense1vars={'weights':tf.Variable(tf.random_normal([(depthstart*8)*16*16, dense1nodes],stddev=stvs)), \n",
    "                'biases':tf.Variable(tf.random_normal([dense1nodes],stddev=stvs))}\n",
    "        \n",
    "        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, dense1vars['weights'])\n",
    "        \n",
    "        dense1=tf.add(tf.matmul(flatten,dense1vars['weights']),dense1vars['biases'])\n",
    "        dense1=tf.nn.relu(dense1)\n",
    "        dropout1=tf.nn.dropout(dense1,keepprob)\n",
    "        \n",
    "    with tf.name_scope(\"Dense2\"):\n",
    "        dense2nodes=1000\n",
    "        dense2vars={'weights':tf.Variable(tf.random_normal([dense1nodes, dense2nodes],stddev=stvs)), \n",
    "                'biases':tf.Variable(tf.random_normal([dense2nodes],stddev=stvs))}\n",
    "        \n",
    "        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, dense2vars['weights'])\n",
    "        dense2=tf.add(tf.matmul(dropout1,dense2vars['weights']),dense2vars['biases'])\n",
    "        dense2=tf.nn.relu(dense2)\n",
    "        dropout2=tf.nn.dropout(dense2,keepprob)\n",
    "    \n",
    "    #Softmax for prediction\n",
    "    with tf.name_scope(\"Out_Layer\"):\n",
    "        outvars={'weights':tf.Variable(tf.random_normal([dense2nodes, num_classes],stddev=stvs)), \n",
    "                'biases':tf.Variable(tf.random_normal([num_classes],stddev=stvs))}\n",
    "        \n",
    "        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, outvars['weights'])\n",
    "        outlayer=tf.add(tf.matmul(dropout2,outvars['weights']),outvars['biases'])\n",
    "        print(outlayer)\n",
    "        \n",
    "    return outlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve true class\n",
    "def Get_Ground(in_val,num_classes_tf_in):\n",
    "    if not in_val.dtype == tf.float32:\n",
    "        in_val = tf.cast( in_val, tf.float32)\n",
    "        \n",
    "    if not num_classes_tf_in.dtype == tf.int32:\n",
    "        num_classes_tf_in = tf.cast(num_classes_tf_in, tf.int32)\n",
    "        \n",
    "    max_val=tf.reduce_max(in_val)\n",
    "    \n",
    "    if max_val==0:\n",
    "        return tf.one_hot(in_val,num_classes_tf_in)\n",
    "    else:\n",
    "        norm_val=tf.divide(in_val,max_val)\n",
    "        return tf.one_hot(tf.cast(norm_val,tf.uint8),num_classes_tf_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define flow graph\n",
    "logits = conv_net(X, inputdepth, trainmode)\n",
    "\n",
    "with tf.name_scope(\"Get_Label\"):\n",
    "    label_1H=Get_Ground(true_label,num_classes_tf)\n",
    "\n",
    "#Prediction function for evaluating accuracy\n",
    "with tf.name_scope(\"Softmax\"):\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "#Define Loss\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    loss_op=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "    \n",
    "    #regularizer = tf.contrib.layers.l2_regularizer(scale=0.0001)\n",
    "    #reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "    #loss_op += reg_term\n",
    "    \n",
    "    tf.summary.scalar(\"Loss\",loss_op)\n",
    "\n",
    "#Define optimizer\n",
    "with tf.name_scope(\"Optimizer\"):\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)   \n",
    "    \n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "with tf.name_scope(\"Accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\",accuracy)\n",
    "\n",
    "#Confusion matrix\n",
    "with tf.name_scope(\"Confusion_Matrix\"):\n",
    "    batch_confusion = tf.confusion_matrix(tf.reshape(tf.argmax(Y,axis=1),[-1]),tf.reshape(tf.argmax(prediction,axis=1),[-1]),num_classes=num_classes,name='batch_confusion')\n",
    "    confusion_image = tf.reshape( tf.cast(batch_confusion, tf.float32),[1, num_classes, num_classes, 1])\n",
    "    tf.summary.image('confusion',confusion_image)\n",
    "\n",
    "#Define writer for Tensorboard\n",
    "writer=tf.summary.FileWriter(\"/output/3\")\n",
    "summ=tf.summary.merge_all()\n",
    "\n",
    "# Initialize the variablesS\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Define saver for model saver\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send it\n",
    "ct=0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    #saver.restore(sess, '/models/model_30112') #Option to restore a model to resume training\n",
    "    \n",
    "    for epoch in range(hm_epochs):\n",
    "        \n",
    "        x = ([[i] for i in range(numfiles)])\n",
    "        shuflist=random.sample(x,len(x))\n",
    "        tempfiles_new=[]\n",
    "        s=''\n",
    "        for i in range(numfiles):\n",
    "            if len(str(shuflist[i][0]+1))==1:\n",
    "                templist=('0000', str(shuflist[i][0]+1)) \n",
    "                tempfiles_new.append(s.join(templist))\n",
    "            elif len(str(shuflist[i][0]+1))==2:\n",
    "                templist=('000', str(shuflist[i][0]+1)) \n",
    "                tempfiles_new.append(s.join(templist)) \n",
    "            elif len(str(shuflist[i][0]+1))==3:\n",
    "                templist=('00', str(shuflist[i][0]+1)) \n",
    "                tempfiles_new.append(s.join(templist))  \n",
    "            elif len(str(shuflist[i][0]+1))==4:\n",
    "                templist=('0', str(shuflist[i][0]+1)) \n",
    "                tempfiles_new.append(s.join(templist)) \n",
    "            else:\n",
    "                templist=(str(shuflist[i][0]+1)) \n",
    "                tempfiles_new.append(s.join(templist))\n",
    "\n",
    "\n",
    "        i=0\n",
    "        print(\"EPOCH \" + \"{:d}\".format(epoch))\n",
    "        while i+BS<numfiles:    \n",
    "            start=i; end=i+BS;\n",
    "            bx=np.empty((BS,picsize,picsize,inputdepth))\n",
    "            by=np.empty((BS,num_classes))\n",
    "            ground=np.empty((BS,picsize,picsize))\n",
    "            for j in range(BS): # Import data for that mini-batch\n",
    "                bx[j,:,:,0]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'A.png')) #Import CT slice\n",
    "                bx[j,:,:,1]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'B.png')) #Import CT slice\n",
    "                bx[j,:,:,2]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'C.png')) #Import CT slice\n",
    "                bx[j,:,:,3]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'D.png')) #Import CT slice\n",
    "                bx[j,:,:,4]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'E.png')) #Import CT slice\n",
    "                bx[j,:,:,5]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'F.png')) #Import CT slice\n",
    "                bx[j,:,:,6]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'G.png')) #Import CT slice\n",
    "                \n",
    "                bx[j,:,:,:]=bx[j,:,:,:]-np.mean(bx[j,:,:,:],axis=(0,1,2))\n",
    "\n",
    "                by[j,:]=sess.run(label_1H,feed_dict={true_label: np.array(classtrue[int(tempfiles_new[i+j])-1][0]).reshape(1)}) #Create one hot tensor   \n",
    "                \n",
    "\n",
    "            #After setting up batch_x and batch_y, we can run train_op\n",
    "            sess.run(train_op,feed_dict={X: bx, Y: by})\n",
    "            loss, acc, summary=sess.run([loss_op,accuracy,summ],feed_dict={X: bx, Y: by})\n",
    "            \n",
    "            \n",
    "            #conf_im=sess.run(confusion_image,feed_dict={X: bx, Y: by})\n",
    "            writer.add_summary(summary, ct)\n",
    "            \n",
    "            \n",
    "            if i%display_step==0 or i==BS or i==0: \n",
    "                s=''\n",
    "                \n",
    "                checkpointnamelist=('./model_',str(ct))\n",
    "                checkpointname= s.join(checkpointnamelist)\n",
    "                save_path = saver.save(sess,checkpointname)\n",
    "                \n",
    "                #print(\"Model saved in file: %s\" % save_path)\n",
    "                print(\" Step \" + str(i) + \", Minibatch Loss= \" + \"{:.4f}\".format(loss) + \", Training Accuracy = \" + \"{:.3f}\".format(acc)) \n",
    "                print(sess.run(batch_confusion,feed_dict={X: bx, Y: by}))\n",
    "                \n",
    "            if i%(display_step*5)==0 and i>0:\n",
    "                #Validation step\n",
    "                acc_vec_val=[]; ct_val=0;\n",
    "                for j in range(numfiles_val): #Import data for that mini-batch\n",
    "                    #for j in range(100): #Import data for that mini-batch\n",
    "\n",
    "                    #Validation step -- Load entire validation set\n",
    "                    bx=np.empty((1,picsize,picsize,inputdepth))\n",
    "                    by=np.empty((1,num_classes))\n",
    "                    batch_ID=0;\n",
    "                    #Import input data\n",
    "                    ID=str(j)\n",
    "                    while(len(ID))<5:\n",
    "                        ID='0'+ID\n",
    "\n",
    "                    bx[j,:,:,0]=imageio.imread((imdpath_val + ID + 'A.png')) #Import CT slice\n",
    "                    bx[j,:,:,1]=imageio.imread((imdpath_val + ID + 'B.png')) #Import CT slice\n",
    "                    bx[j,:,:,2]=imageio.imread((imdpath_val + ID + 'C.png')) #Import CT slice\n",
    "                    bx[j,:,:,3]=imageio.imread((imdpath_val + ID + 'D.png')) #Import CT slice\n",
    "                    bx[j,:,:,4]=imageio.imread((imdpath_val + ID + 'E.png')) #Import CT slice\n",
    "                    bx[j,:,:,5]=imageio.imread((imdpath_val + ID + 'F.png')) #Import CT slice\n",
    "                    bx[j,:,:,6]=imageio.imread((imdpath_val + ID + 'G.png')) #Import CT slice        \n",
    "                    bx[batch_ID,:,:,:]=bx[batch_ID,:,:,:]-np.mean(bx[batch_ID,:,:,:],axis=(0,1,2))\n",
    "\n",
    "                    by[batch_ID,:]=ground_val[int(ID)]  \n",
    "                    #After setting up batch_x and batch_y, we can run train_op\n",
    "                    loss, acc, summary=sess.run([loss_op,accuracy,summ],feed_dict={X: bx, Y: by})\n",
    "                    BC=sess.run(batch_confusion,feed_dict={X: bx, Y: by})\n",
    "                    acc_vec_val.append(acc)\n",
    "                    \n",
    "                print(\"Validation Accuracy: \" + \"{:.4f}\".format(np.mean(acc_vec_val)))\n",
    "\n",
    "                \n",
    "            i=i+BS\n",
    "            ct=ct+BS\n",
    "        \n",
    "    print(\"We out here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmode='yes'\n",
    "if testmode=='yes':\n",
    "    imdpath=\"/mydata/ImdSag/\"\n",
    "    imdfiles=glob.glob(\"/mydata/ImdSag/*.png\")\n",
    "    numfiles=int(len(glob.glob(\"/mydata/ImdSag/*.png\"))/inputdepth)\n",
    "    root='/mydata/'\n",
    "    print(numfiles)\n",
    "    x = ([[i] for i in range(numfiles)])\n",
    "    shuflist=random.sample(x,len(x))\n",
    "    tempfiles_new=[]\n",
    "    s=''\n",
    "    for i in range(numfiles):\n",
    "        if len(str(shuflist[i][0]+1))==1:\n",
    "            templist=('0000', str(shuflist[i][0]+1)) \n",
    "            tempfiles_new.append(s.join(templist))\n",
    "        elif len(str(shuflist[i][0]+1))==2:\n",
    "            templist=('000', str(shuflist[i][0]+1)) \n",
    "            tempfiles_new.append(s.join(templist)) \n",
    "        elif len(str(shuflist[i][0]+1))==3:\n",
    "            templist=('00', str(shuflist[i][0]+1)) \n",
    "            tempfiles_new.append(s.join(templist))  \n",
    "        elif len(str(shuflist[i][0]+1))==4:\n",
    "            templist=('0', str(shuflist[i][0]+1)) \n",
    "            tempfiles_new.append(s.join(templist)) \n",
    "        else:\n",
    "            templist=(str(shuflist[i][0]+1)) \n",
    "            tempfiles_new.append(s.join(templist))\n",
    "\n",
    "    print(tempfiles_new[:10])\n",
    "    print((imdpath + str(tempfiles_new[0]) + 'A.png'))\n",
    "        \n",
    "        \n",
    "    #Sess time\n",
    "    TP=0; TN=0; FP=0; FN=0;\n",
    "    ct=0\n",
    "    BS=32\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver.restore(sess, '/models/model_87232')\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        i=0\n",
    "        print(\"Testing\")\n",
    "        while i+BS<numfiles+1:    \n",
    "            start=i; end=i+BS;\n",
    "            bx=np.empty((BS,picsize,picsize,inputdepth))\n",
    "            by=np.empty((BS,num_classes))\n",
    "            ground=np.empty((BS,picsize,picsize)) \n",
    "            for j in range(BS): # Import data for that mini-batch\n",
    "                bx[j,:,:,0]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'A.png')) #Import CT slice\n",
    "                bx[j,:,:,1]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'B.png')) #Import CT slice\n",
    "                bx[j,:,:,2]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'C.png')) #Import CT slice\n",
    "                bx[j,:,:,3]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'D.png')) #Import CT slice\n",
    "                bx[j,:,:,4]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'E.png')) #Import CT slice\n",
    "                bx[j,:,:,5]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'F.png')) #Import CT slice\n",
    "                bx[j,:,:,6]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'G.png')) #Import CT slice\n",
    "\n",
    "                by[j,:]=sess.run(label_1H,feed_dict={true_label: np.array(classtrue[int(tempfiles_new[i+j])-1][0]).reshape(1)}) #Create one hot tensor   \n",
    "\n",
    "            acc, conf=sess.run([accuracy,batch_confusion],feed_dict={X: bx, Y: by})\n",
    "            print(acc)\n",
    "            print(conf)\n",
    "            TP+=conf[1,1]\n",
    "            TN+=conf[0,0]\n",
    "            \n",
    "            FP+=conf[1,0]\n",
    "            FN+=conf[0,1]\n",
    "            i=i+BS\n",
    "        \n",
    "        BS=26\n",
    "        while i+BS<numfiles+1:    \n",
    "            start=i; end=i+BS;\n",
    "            bx=np.empty((BS,picsize,picsize,inputdepth))\n",
    "            by=np.empty((BS,num_classes))\n",
    "            ground=np.empty((BS,picsize,picsize)) \n",
    "            for j in range(BS): # Import data for that mini-batch\n",
    "                bx[j,:,:,0]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'A.png')) #Import CT slice\n",
    "                bx[j,:,:,1]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'B.png')) #Import CT slice\n",
    "                bx[j,:,:,2]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'C.png')) #Import CT slice\n",
    "                bx[j,:,:,3]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'D.png')) #Import CT slice\n",
    "                bx[j,:,:,4]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'E.png')) #Import CT slice\n",
    "                bx[j,:,:,5]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'F.png')) #Import CT slice\n",
    "                bx[j,:,:,6]=imageio.imread((imdpath + str(tempfiles_new[i+j]) + 'G.png')) #Import CT slice\n",
    "\n",
    "                by[j,:]=sess.run(label_1H,feed_dict={true_label: np.array(classtrue[int(tempfiles_new[i+j])-1][0]).reshape(1)}) #Create one hot tensor   \n",
    "\n",
    "            acc, conf=sess.run([accuracy,batch_confusion],feed_dict={X: bx, Y: by})\n",
    "            TP+=conf[1,1]\n",
    "            TN+=conf[0,0]\n",
    "            FP+=conf[1,0]\n",
    "            FN+=conf[0,1]\n",
    "            i=i+BS\n",
    "            \n",
    "    print((TP+TN)/(TP+TN+FP+FN))\n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
