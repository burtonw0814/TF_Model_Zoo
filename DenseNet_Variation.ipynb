{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import misc\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "\n",
    "#Huang G, Liu Z, Weinberger KQ, van der Maaten L. Densely connected convolutional networks. ...\n",
    "#InProceedings of the IEEE conference on computer vision and pattern recognition 2017 Jul 1 (Vol. 1, No. 2, p. 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_files(num_files):\n",
    "    x = ([[i] for i in range(num_files)])\n",
    "    shuflist=random.sample(x,len(x))\n",
    "    list_files=[]\n",
    "\n",
    "    s=''\n",
    "    for i in range(num_files):\n",
    "        ID=str(shuflist[i][0])\n",
    "        while len(ID)<5:\n",
    "            ID='0'+ID\n",
    "        list_files.append(ID)\n",
    "        \n",
    "        \n",
    "    return list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_translate(input_block,y_dir,x_dir):\n",
    "    imsize=input_block.shape\n",
    "    temp=np.empty((imsize[0],imsize[1],imsize[2]))\n",
    "    y_abs=np.absolute(y_dir)\n",
    "    x_abs=np.absolute(x_dir)\n",
    "    \n",
    "    #Vertical translation\n",
    "    if y_dir != 0: \n",
    "        height_pad=np.zeros((y_abs,imsize[1],imsize[2]))\n",
    "        if y_dir>0: #Shift up --> attach zeros to bottom\n",
    "            temp=np.concatenate((input_block[y_abs:,:,:],height_pad),axis=0)\n",
    "        else: #Shift down --> attach zeros to top\n",
    "            temp=np.concatenate((height_pad,input_block[:(-1*y_abs),:,:]),axis=0)\n",
    "    else: \n",
    "        temp=input_block\n",
    "        \n",
    "    #Horizontal translation\n",
    "    if x_dir!=0:\n",
    "        width_pad=np.zeros((imsize[0],x_abs,imsize[2]))\n",
    "        if x_dir>0:\n",
    "            temp2=np.concatenate((width_pad,temp[:,:(-1*x_abs),:]),axis=1)\n",
    "        else: \n",
    "            temp2=np.concatenate((temp[:,x_abs:,:],width_pad),axis=1)\n",
    "    else:\n",
    "        temp2=temp\n",
    "        \n",
    "    return temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of files in training set and also median frequency class weights for cross entropy loss\n",
    "def get_num_files(path_ground):  \n",
    "    s=''\n",
    "    classtrue=[]\n",
    "    f = open(path_ground, 'r')\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row[0]=='0':\n",
    "            sublist=(1,0)\n",
    "        else:\n",
    "            sublist=(0,1)\n",
    "    \n",
    "        classtrue.append(sublist)\n",
    "    f.close()\n",
    "    ground_vec=classtrue\n",
    "\n",
    "    return len(ground_vec), ground_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer wrappers\n",
    "def conv_layer(inputs, channels_in, channels_out, is_training, pad_val='SAME', stvs=0.01, filter_size=1, strides=1, scopename=\"conv\"):\n",
    "    with tf.name_scope(scopename):\n",
    "        s=''; weightname=(scopename,'_weights'); biasname=(scopename,'_bias')\n",
    "        \n",
    "        w=tf.Variable(tf.random_normal([filter_size, filter_size, channels_in, channels_out],stddev=stvs),name=s.join(weightname))\n",
    "        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, w)\n",
    "        b=tf.Variable(tf.random_normal([channels_out],stddev=stvs),name=s.join(biasname))\n",
    "        tf.summary.histogram((\"Weight\" + scopename),w)\n",
    "        tf.summary.histogram((\"Bias\" + scopename),b)\n",
    "        \n",
    "        x = tf.nn.conv2d(inputs, w, strides=[1, strides, strides, 1], padding=pad_val)\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "        #x=batch_norm_wrapper(x, is_training, decay = 0.99)\n",
    "        #return tf.nn.relu(x)\n",
    "        return x\n",
    "        \n",
    "def BNR(inputs,is_training,scopename=\"BNR\"):\n",
    "    # layer norm + ReLu\n",
    "    with tf.name_scope(scopename):\n",
    "        \n",
    "        x=tf.contrib.layers.layer_norm(inputs)\n",
    "        return tf.nn.relu(x)\n",
    "    \n",
    "def shortcut_function(inputs,channels_in,num_filter_out,stvs=0.01,strides=1,scopename=\"shortcut\"):\n",
    "    with tf.name_scope(scopename):\n",
    "        s=''; weightname=(scopename,'_weights')\n",
    "        filter_size=1\n",
    "        w=tf.Variable(tf.random_normal([filter_size, filter_size, channels_in, num_filter_out],stddev=stvs),name=s.join(weightname))\n",
    "       \n",
    "        shortcut = tf.nn.conv2d(inputs, w, strides=[1, strides, strides, 1], padding='VALID')\n",
    "        \n",
    "        return shortcut\n",
    "    \n",
    "def maxpool2d(x, k=2, stride=2,scopename=\"pool\"):\n",
    "    with tf.name_scope(scopename):\n",
    "        # MaxPool2D wrapper\n",
    "        return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, stride, stride, 1],padding='VALID')\n",
    "\n",
    "def Denseblock(inputs,num_dense_layers,k,channels_current, is_training,scopename='Denseblock'):\n",
    "    \n",
    "    with tf.name_scope(scopename):\n",
    "        # Initialize shortcut list\n",
    "        shortcut=list()\n",
    "        shortcut.append(inputs)\n",
    "    \n",
    "        # Create first dense layer\n",
    "        x = Denselayer(inputs,channels_current,k, is_training,scopename=scopename)\n",
    "        shortcut.append(x); channels_current+=k #Iterate number of current channels\n",
    "    \n",
    "        for i in range(1,num_dense_layers):\n",
    "            x=tf.concat(shortcut,3)\n",
    "            x = Denselayer(x,channels_current,k, is_training,scopename=scopename)\n",
    "            shortcut.append(x); channels_current+=k\n",
    "    \n",
    "        out=tf.concat(shortcut,3)\n",
    "        \n",
    "        return out, channels_current\n",
    "    \n",
    "# Batch normalization wrapper to distinguish between training and testing\n",
    "# REF: https://r2rt.com/implementing-batch-normalization-in-tensorflow.html\n",
    "def batch_norm_wrapper(inputs, is_training, decay = 0.99):\n",
    "    \n",
    "    epsilon = 1e-3\n",
    "    scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "    beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "    #pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-3], inputs.get_shape()[-2] ,inputs.get_shape()[-1]]), trainable=False)\n",
    "    #pop_var = tf.Variable(tf.ones([inputs.get_shape()[-3], inputs.get_shape()[-2] ,inputs.get_shape()[-1]]), trainable=False)\n",
    "    \n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "  \n",
    "    if is_training:\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs,[0,1,2])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs,\n",
    "                batch_mean, batch_var, beta, scale, epsilon)\n",
    "    else:\n",
    "        return tf.nn.batch_normalization(inputs,\n",
    "            pop_mean, pop_var, beta, scale, epsilon)\n",
    "        \n",
    "def Denselayer(inputs,channels_current,k, is_training,scopename='Denselayer'):\n",
    "    \n",
    "    with tf.name_scope(scopename):\n",
    "        x=BNR(inputs,is_training,scopename=scopename)\n",
    "        x=conv_layer(x,channels_current,4*k, is_training,filter_size=1,scopename=scopename)\n",
    "    \n",
    "        x=BNR(x,is_training,scopename=scopename)\n",
    "        x=conv_layer(x,4*k,k, is_training,filter_size=3,scopename=scopename)\n",
    "    \n",
    "        return x\n",
    "         \n",
    "def Transitionlayer(inputs, theta, channels_current, is_training,scopename='Tranny'):\n",
    "    \n",
    "    with tf.name_scope(scopename):\n",
    "     \n",
    "        # Conv\n",
    "        x=conv_layer(inputs,channels_current,int(channels_current*theta), is_training,filter_size=1,scopename=scopename)\n",
    "        channels_current=channels_current*theta\n",
    "    \n",
    "        # Pooling\n",
    "        avg_pool=tf.nn.pool(x, [2,2], \"AVG\", \"SAME\",strides=[2,2])\n",
    "    \n",
    "        return avg_pool, int(channels_current)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet(x, inputdepth, picsize, is_training):\n",
    "    depth_start=64\n",
    "    num_classes=2\n",
    "    inputdepth=1\n",
    "    stvs=0.01\n",
    "    k=12\n",
    "    theta=0.5\n",
    "    if is_training==True:\n",
    "        keepprob=0.5\n",
    "    else:\n",
    "        keepprob=1\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, picsize, picsize, inputdepth]) \n",
    "    \n",
    "    # Initial conv_block\n",
    "    with tf.name_scope(\"Initial_Conv\"):\n",
    "        conv_initial = conv_layer(x, inputdepth, 2*k, is_training, filter_size=7, scopename=\"conv_initial\")\n",
    "        channels_current=2*k\n",
    "    \n",
    "    # Initial max pooling\n",
    "    with tf.name_scope(\"Max_Pool1\"):\n",
    "        # From 256 down to 128\n",
    "        pool_initial = maxpool2d(conv_initial,scopename=\"pool_initial\")\n",
    "          \n",
    "    # Dense block 1\n",
    "    scope_in=\"Denseblock1\"\n",
    "    with tf.name_scope(scope_in):\n",
    "        print(scope_in)\n",
    "        num_dense_layers=6\n",
    "        x, channels_current=Denseblock(pool_initial,num_dense_layers,k,channels_current, is_training,scopename=scope_in)\n",
    "        print(channels_current)\n",
    "        \n",
    "    # Transition 1\n",
    "    scope_in=\"Tranny1\"\n",
    "    with tf.name_scope(scope_in):\n",
    "        # From 128 to 64\n",
    "        print(scope_in)\n",
    "        x, channels_current=Transitionlayer(x, theta, channels_current, is_training,scopename=scope_in)\n",
    "        print(channels_current)\n",
    "        \n",
    "    # Dense block 2\n",
    "    scope_in=\"Denseblock2\"\n",
    "    with tf.name_scope(scope_in):\n",
    "        scope_in=scope_in\n",
    "        print(scope_in)\n",
    "        num_dense_layers=12\n",
    "        x, channels_current=Denseblock(x,num_dense_layers,k,channels_current, is_training,scopename=scope_in)\n",
    "        print(channels_current)\n",
    "        \n",
    "    # Transition 2\n",
    "    scope_in=\"Tranny2\"\n",
    "    with tf.name_scope(scope_in):\n",
    "        # From 64 to 32\n",
    "        print(scope_in)\n",
    "        x, channels_current=Transitionlayer(x, theta, channels_current, is_training,scopename=scope_in)\n",
    "        print(channels_current)\n",
    "        \n",
    "    # Dense block 3\n",
    "    with tf.name_scope(scope_in):\n",
    "        scope_in=\"Denseblock3\"\n",
    "        print(scope_in)\n",
    "        num_dense_layers=24\n",
    "        x, channels_current=Denseblock(x,num_dense_layers,k,channels_current, is_training,scopename=scope_in)\n",
    "        print(channels_current)\n",
    "        \n",
    "    # Transition 3\n",
    "    scope_in=\"Tranny3\"\n",
    "    with tf.name_scope(scope_in):\n",
    "        # From 32 to 16\n",
    "        print(scope_in)\n",
    "        x, channels_current=Transitionlayer(x, theta, channels_current, is_training,scopename=scope_in)\n",
    "        print(channels_current)\n",
    "        \n",
    "    # Dense block 4     \n",
    "    scope_in=\"Denseblock4\"\n",
    "    with tf.name_scope(scope_in):\n",
    "        print(scope_in)\n",
    "        num_dense_layers=16\n",
    "        x, channels_current=Denseblock(x,num_dense_layers,k,channels_current, is_training,scopename=scope_in)\n",
    "        print(channels_current)\n",
    "    \n",
    "    # Average pool\n",
    "    with tf.name_scope(\"Average_Pool_final\"):\n",
    "        # From 16 to 8\n",
    "        avg_pool=tf.nn.pool(x, [2,2], \"AVG\", \"SAME\",strides=[2,2])\n",
    "    \n",
    "  \n",
    "    # Fully connected\n",
    "    with tf.name_scope(\"Dense\"):\n",
    "        #Flatten for fully connected\n",
    "        print(channels_current)\n",
    "        print((picsize/(2**5)))\n",
    "        nodes_in=int(channels_current*(picsize/(2**5))*(picsize/(2**5)))\n",
    "        print(nodes_in)\n",
    "        flatten=tf.reshape(avg_pool,[-1,nodes_in])\n",
    "        densenodes=1000\n",
    "        densevars={'weights':tf.Variable(tf.random_normal([nodes_in, densenodes],stddev=stvs)), \n",
    "                'biases':tf.Variable(tf.random_normal([densenodes],stddev=stvs))}\n",
    "        \n",
    "        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, densevars['weights'])\n",
    "        dense=tf.add(tf.matmul(flatten,densevars['weights']),densevars['biases'])\n",
    "        #dense=tf.nn.relu(dense)\n",
    "        dropout=tf.nn.dropout(dense,keepprob)\n",
    "        \n",
    "    # Compress to num_classes for prediction\n",
    "    with tf.name_scope(\"Out_Layer\"):\n",
    "        outvars={'weights':tf.Variable(tf.random_normal([densenodes, num_classes],stddev=stvs)), \n",
    "                'biases':tf.Variable(tf.random_normal([num_classes],stddev=stvs))}\n",
    "        \n",
    "        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, outvars['weights'])\n",
    "        outlayer=tf.add(tf.matmul(dropout,outvars['weights']),outvars['biases'])\n",
    "        print(outlayer)\n",
    "        \n",
    "    return outlayer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(trainmode, inputdepth, num_classes, picsize):\n",
    "\n",
    "    #Define placeholders\n",
    "    with tf.name_scope(\"Input\"):\n",
    "        X = tf.placeholder(tf.float32, [None, picsize, picsize, inputdepth])\n",
    "    with tf.name_scope(\"Ground_Truth\"):\n",
    "        Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "    \n",
    "    #Define flow graph\n",
    "    logits = densenet(X, inputdepth, picsize, trainmode) ###<---IS TRAINING\n",
    "\n",
    "    #Prediction function for evaluating accuracy\n",
    "    with tf.name_scope(\"Softmax\"):\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    #Define Loss\n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        loss_op=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=0.0001)\n",
    "        reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "        loss_op += reg_term\n",
    "\n",
    "        tf.summary.scalar(\"Loss\",loss_op)\n",
    "\n",
    "    # Define optimizer\n",
    "    with tf.name_scope(\"Optimizer\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)   \n",
    "\n",
    "        train_op = optimizer.minimize(loss_op)\n",
    "        \n",
    "    # Evaluate model\n",
    "    with tf.name_scope(\"Accuracy\"):\n",
    "        correct_pred = tf.equal(tf.argmax(prediction, axis=1), tf.argmax(Y, axis=1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\",accuracy)\n",
    "\n",
    "    #Confusion matrix\n",
    "    with tf.name_scope(\"Confusion_Matrix\"):\n",
    "        batch_confusion = tf.confusion_matrix(tf.reshape(tf.argmax(Y,axis=1),[-1]),tf.reshape(tf.argmax(prediction,axis=1),[-1]),num_classes=num_classes,name='batch_confusion')\n",
    "        confusion_image = tf.reshape( tf.cast(batch_confusion, tf.float32),[1, num_classes, num_classes, 1])\n",
    "        tf.summary.image('confusion',confusion_image)\n",
    "\n",
    "    #Define writer for Tensorboard\n",
    "    writer=tf.summary.FileWriter(\"/output/10\")\n",
    "    writer_val=tf.summary.FileWriter(\"/output/4\")\n",
    "    summ=tf.summary.merge_all()\n",
    "\n",
    "    # Initialize the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    #Define saver for model saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    \n",
    "    return X, Y,  logits, prediction, loss_op, train_op, accuracy, batch_confusion, confusion_image, writer, writer_val, summ, init, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.00001\n",
    "batch_size = 32\n",
    "display_step = batch_size*10\n",
    "hm_epochs=30\n",
    "BS=batch_size\n",
    "picsize=256\n",
    "inputdepth=1\n",
    "num_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_mode=True\n",
    "tf.reset_default_graph()\n",
    "X, Y, logits, prediction, loss_op, train_op, accuracy, batch_confusion, confusion_image, writer, writer_val, summ, init, saver = build_graph(train_mode, inputdepth, num_classes, picsize)\n",
    "\n",
    "print('Graph built')\n",
    "\n",
    "\n",
    "mode='j'\n",
    "if mode=='j':    \n",
    "    imdpath=\"/path/to/dir/Knee_MRI/Femur/Train/ImdTrain/\"\n",
    "    pxdpath=\"/path/to/label/csv/fem_mri_classlist_Train.csv\"\n",
    "\n",
    "    imdpath_val=\"/path/to/dir/Knee_MRI/Femur/Val/ImdVal/\"\n",
    "    pxdpath_val=\"/path/to/label/csv/fem_mri_classlist_Val.csv\"\n",
    "    \n",
    "    imdpath_test=\"/path/to/dir/Knee_MRI/Femur/Test/ImdTest/\"\n",
    "    pxdpath_test=\"/path/to/label/csv/Knee_MRI/Femur/Test/fem_mri_classlist_Test.csv\"\n",
    "    \n",
    "else:\n",
    "    imdpath=\"/mydata/Femur/Train/ImdTrain/\"\n",
    "    pxdpath=\"/mydata/Femur/Train/fem_mri_classlist_Train.csv\"\n",
    "\n",
    "    imdpath_val=\"/mydata/Femur/Val/ImdVal/\"\n",
    "    pxdpath_val=\"/mydata/Femur/Val/fem_mri_classlist_Val.csv\"\n",
    "    \n",
    "    imdpath_test=\"/mydata/Femur/Test/ImdTest/\"\n",
    "    pxdpath_test=\"/mydata/Femur/Test/fem_mri_classlist_Test.csv\"\n",
    "    \n",
    "# Class data for classification of slices is stored as either 0 or 1 (not a femur, or femur) in a .csv \n",
    "# Where each row corresponds to slice's file identifier in the data store\n",
    "\n",
    "num_files_train, ground_train = get_num_files(pxdpath)\n",
    "\n",
    "num_files_val, ground_val = get_num_files(pxdpath_val)\n",
    "\n",
    "num_files_test, ground_test = get_num_files(pxdpath_test)\n",
    "\n",
    "\n",
    "print(len(ground_train))\n",
    "print(ground_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send it\n",
    "ct=0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer.add_graph(sess.graph)\n",
    "    #saver.restore(sess, '/models/model_30112') #Option to restore a model to resume training\n",
    "    \n",
    "    for epoch in range(hm_epochs):\n",
    "        \n",
    "        #List of strings of random numbers with correct # characters\n",
    "        file_list=random_files(num_files_train)\n",
    "\n",
    "        i=0\n",
    "        print(\"EPOCH \" + \"{:d}\".format(epoch))\n",
    "        while i+BS<num_files_train:    \n",
    "            start=i; end=i+BS;\n",
    "            bx=np.empty((BS,picsize,picsize,inputdepth))\n",
    "            by=np.empty((BS,num_classes))\n",
    "            for j in range(BS): # Import data for that mini-batch\n",
    "                bx[j,:,:,0]=imageio.imread((imdpath + file_list[i+j] + 'D.png')) #Import CT slice\n",
    "                bx[j,:,:,0]=bx[j,:,:,0]-np.mean(bx[j,:,:,0],axis=(0,1))\n",
    "                \n",
    "                #Apply translation augmentation to input block\n",
    "                x_stvs=15; y_stvs=15; \n",
    "                x_t=int(x_stvs*np.random.randn()); y_t=int(y_stvs*np.random.randn());\n",
    "                bx[j,:,:,:]=random_translate(bx[j,:,:,:],x_t,y_t)\n",
    "                \n",
    "                by[j,:]=ground_train[int(file_list[i+j])]\n",
    "\n",
    "            #After setting up batch_x and batch_y, we can run train_op\n",
    "            sess.run(train_op,feed_dict={X: bx, Y: by})\n",
    "            loss, acc, summary=sess.run([loss_op,accuracy,summ],feed_dict={X: bx, Y: by})\n",
    "            \n",
    "            #conf_im=sess.run(confusion_image,feed_dict={X: bx, Y: by})\n",
    "            writer.add_summary(summary, ct)\n",
    "            \n",
    "            if i%display_step==0 and i>0: \n",
    "                s=''\n",
    "                \n",
    "                checkpointnamelist=('./model_',str(ct))\n",
    "                checkpointname= s.join(checkpointnamelist)\n",
    "                save_path = saver.save(sess,checkpointname)\n",
    "                \n",
    "                #print(\"Model saved in file: %s\" % save_path)\n",
    "                print(\" Step \" + str(i) + \", Minibatch Loss= \" + \"{:.4f}\".format(loss) + \", Training Accuracy = \" + \"{:.3f}\".format(acc)) \n",
    "                print(sess.run(batch_confusion,feed_dict={X: bx, Y: by}))\n",
    "                \n",
    "                #Show example image from batch with ground truth\n",
    "                #temp=np.squeeze(bx[0,:,:,:])\n",
    "                #plt.imshow(temp,cmap='gray')\n",
    "                #plt.show()\n",
    "                #print(by[0,:])\n",
    "    \n",
    "                \n",
    "            if i%(5*display_step)==0 and i>0:    \n",
    "            \n",
    "                acc_vec_val=[]; ct_val=0;\n",
    "                #for j in range(num_files_val): #Import data for that mini-batch\n",
    "                for j in range(100): #Import data for that mini-batch\n",
    "\n",
    "                    #Validation step -- Load entire validation set\n",
    "                    bx=np.empty((1,picsize,picsize,inputdepth))\n",
    "                    by=np.empty((1,num_classes))\n",
    "                    batch_ID=0;\n",
    "                    #Import input data\n",
    "                    ID=str(j)\n",
    "                    while(len(ID))<5:\n",
    "                        ID='0'+ID\n",
    "\n",
    "                    bx[batch_ID,:,:,0]=imageio.imread((imdpath_val + ID + 'D.png')) #Import CT slice         \n",
    "                    bx[batch_ID,:,:,0]=bx[batch_ID,:,:,0]-np.mean(bx[batch_ID,:,:,0],axis=(0,1))\n",
    "\n",
    "                    by[batch_ID,:]=ground_val[int(ID)]  \n",
    "                    #After setting up batch_x and batch_y, we can run train_op\n",
    "                    loss, acc, summary=sess.run([loss_op,accuracy,summ],feed_dict={X: bx, Y: by})\n",
    "                    BC=sess.run(batch_confusion,feed_dict={X: bx, Y: by})\n",
    "                    acc_vec_val.append(acc)\n",
    "\n",
    "                print(\"Validation Accuracy: \" + \"{:.4f}\".format(np.mean(acc_vec_val)))\n",
    "\n",
    "                \n",
    "            i=i+BS\n",
    "            ct=ct+BS\n",
    "        \n",
    "    print(\"We out here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctct=0\n",
    "trainstep_actual=0\n",
    "\n",
    "acc_vec=[] #Vector to store accuracy for each image\n",
    "yescount=0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, '/models/model_84736') #Import trained model\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    i=0\n",
    "    for j in range(num_files_test): #Import data for that mini-batch\n",
    "\n",
    "        #Validation step -- Load entire validation set\n",
    "        bx=np.empty((1,picsize,picsize,inputdepth))\n",
    "        by=np.empty((1,num_classes))\n",
    "        batch_ID=0;\n",
    "        #Import input data\n",
    "        ID=str(j)\n",
    "        while(len(ID))<5:\n",
    "            ID='0'+ID\n",
    "            \n",
    "        bx[batch_ID,:,:,0]=imageio.imread((imdpath_test + ID + 'D.png')) #Import CT slice         \n",
    "        bx[batch_ID,:,:,0]=bx[batch_ID,:,:,0]-np.mean(bx[batch_ID,:,:,0],axis=(0,1))\n",
    "\n",
    "        by[batch_ID,:]=ground_test[int(ID)]  \n",
    "        #After setting up batch_x and batch_y, we can run train_op\n",
    "        loss, acc, summary=sess.run([loss_op,accuracy,summ],feed_dict={X: bx, Y: by})\n",
    "        if acc==0:\n",
    "            temp=np.squeeze(bx[0,:,:,:])\n",
    "            plt.imshow(temp,cmap='gray')\n",
    "            plt.show()\n",
    "            print(by)\n",
    "                \n",
    "        BC=sess.run(batch_confusion,feed_dict={X: bx, Y: by})\n",
    "        acc_vec.append(acc)\n",
    "\n",
    "    print(\"Test Accuracy: \" + \"{:.4f}\".format(np.mean(acc_vec)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
